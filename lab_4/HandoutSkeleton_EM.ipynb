{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import OrderedDict\n",
    "from sklearn import datasets, metrics\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import Handout_MNIST.MNIST as mn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell you will have to add a suitable intialisation for the EM-algorithm to find good \"clusters\"\n",
    "# note that \"cluster\" is a bit misleading, as only the degree of belonging to a distribution is evaluated \n",
    "# for each sample, and not an absolute assignment to a cluster is made\n",
    "\n",
    "class EMClusters:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__numOfClasses = 0\n",
    "        self.__numOfAttributes = 0\n",
    "\n",
    "    # Initialises for EM to work\n",
    "    # should be called with a suitable smoothing factor blur, but the default does at least something.\n",
    "    def initialise(self, data, num_classes, num_attributes, blur=0.1):\n",
    "        self.__blur = blur\n",
    "        self.__data = data\n",
    "        self.__numOfAttributes = num_attributes\n",
    "  \n",
    "        self.__numOfClasses = num_classes\n",
    "\n",
    "        self.__clusterMeansAndCovs = np.ones((self.__numOfClasses, self.__numOfAttributes, 2), dtype=float)\n",
    "        self.__priors = np.ones(self.__numOfClasses)\n",
    "\n",
    "        # start of your code\n",
    "        ###\n",
    "        #\n",
    "        # Here, you should now fill in something more sensible, as the code above is only a placeholder solution...\n",
    "        # You need to find initial clusters and set start values for the priors, means and (co)variances that would\n",
    "        # end up in the following arrays:\n",
    "        #\n",
    "        # self.__priors              - is of dimension K x 1 and holds K values, K the number of clusters / classes\n",
    "        # self.__clusterMeansAndCovs - is of dimensions K x I x 2 and holds for each combination of class k\n",
    "        #                              and attribute j the mean in [k, j, 0] and the covariance (only for jj,\n",
    "        #                              as we assume conditional independence given the class) in [k, j, 1]\n",
    "        #\n",
    "        ###\n",
    "        self.__priors = np.random.rand(self.__numOfClasses)\n",
    "        self.__clusterMeansAndCovs = np.random.rand(self.__numOfClasses, self.__numOfAttributes, 2)\n",
    "        # end of your code\n",
    "\n",
    "        \n",
    "    # the actual EM algorithm, should be called with a suitable eps for a stop criterion\n",
    "    # feel free to change (optimize) the implementation, but you do not have to do that\n",
    "    def fit(self, eps=1.0):\n",
    "        epsilon = eps\n",
    "\n",
    "        print(self.__numOfAttributes)\n",
    "\n",
    "        # print(self.__clusterMeansAndCovs[:,1,:])\n",
    "\n",
    "        resp = np.zeros((len(self.__data), self.__numOfClasses))\n",
    "        r_k = np.zeros(self.__numOfClasses)\n",
    "        notDone = True\n",
    "        while notDone:\n",
    "            # print(self.__clusterMeansAndCovs)\n",
    "            # E-step\n",
    "            for i in range(len(self.__data)):\n",
    "                probs = self.__priors.copy()\n",
    "                for k in range(self.__numOfClasses):\n",
    "\n",
    "                    for attr in range(self.__numOfAttributes):\n",
    "                        probs[k] *= 1.0 / np.sqrt(\n",
    "                            2 * math.pi * (self.__clusterMeansAndCovs[k, attr, 1] + self.__blur)) * math.exp(\n",
    "                            -1.0 * math.pow((self.__clusterMeansAndCovs[k, attr, 0] - self.__data[i, attr]), 2) / (\n",
    "                                        self.__clusterMeansAndCovs[k, attr, 1] + self.__blur))\n",
    "\n",
    "                resp[i, :] = probs / sum(probs)\n",
    "                # print(resp[i,:])\n",
    "            \n",
    "            # M-step\n",
    "            newClusterMeansAndCovs = np.zeros((self.__numOfClasses, self.__numOfAttributes, 2))\n",
    "            for k in range(self.__numOfClasses):\n",
    "                r_k[k] = sum(resp[:, k])\n",
    "                self.__priors[k] = 1 / len(self.__data) * r_k[k]\n",
    "\n",
    "                for j in range(self.__numOfAttributes):\n",
    "                    means = sum(resp[:, k] * self.__data[:, j]) / r_k[k]\n",
    "\n",
    "                    newClusterMeansAndCovs[k, j, 0] = means\n",
    "                    newClusterMeansAndCovs[k, j, 1] = sum(resp[:, k] * self.__data[:, j] * self.__data[:, j]) \\\n",
    "                                                      / r_k[k] - newClusterMeansAndCovs[k, j, 0] ** 2\n",
    "\n",
    "            # print(newClusterMeansAndCovs[:, :, 1])\n",
    "\n",
    "            err = np.linalg.norm(self.__clusterMeansAndCovs[:, :, 0] - newClusterMeansAndCovs[:, :, 0])\n",
    "\n",
    "            self.__clusterMeansAndCovs = newClusterMeansAndCovs.copy()\n",
    "            print(\"err = \" + str(err))\n",
    "            if err <= epsilon:\n",
    "                notDone = False\n",
    "                \n",
    "                \n",
    "        # produce \"clusters\", i.e, assign the samples to \"their\" gaussian\n",
    "        clustered = 10 * np.ones(len(self.__data), dtype=int)\n",
    "        for i in range(len(self.__data)):\n",
    "            probs = self.__priors.copy()\n",
    "            for k in range(self.__numOfClasses):\n",
    "\n",
    "                for attr in range(self.__numOfAttributes):\n",
    "                    probs[k] *= 1.0 / np.sqrt(2 * math.pi * (self.__clusterMeansAndCovs[k, attr, 1] + self.__blur)) \\\n",
    "                                * math.exp(\n",
    "                        -1.0 * math.pow((self.__clusterMeansAndCovs[k, attr, 0] - self.__data[i, attr]), 2) \\\n",
    "                        / (self.__clusterMeansAndCovs[k, attr, 1] + self.__blur))\n",
    "\n",
    "            sumProb = np.sum(probs)\n",
    "            if (sumProb > 0.0):\n",
    "                probs = probs / sumProb\n",
    "                clustered[i] = np.argmax(probs)\n",
    "\n",
    "        return clustered, self.__clusterMeansAndCovs\n",
    "\n",
    "    # prediction uses the found gaussians to compute the likelihoods in a GNBC\n",
    "    def predict(self, samples):\n",
    "        predicted = [None for i in range(len(samples))]\n",
    "        prob = [[self.__priors[cls] for cls in range(self.__numOfClasses)] for i in range(len(samples))]\n",
    "        # print(prob)\n",
    "        for i in range(len(samples)):\n",
    "            for cls in range(self.__numOfClasses):\n",
    "                for attr in range(self.__numOfAttributes):\n",
    "                    (mean, var) = (\n",
    "                    self.__clusterMeansAndCovs[cls, attr, 0], self.__clusterMeansAndCovs[cls, attr, 1] + self.__blur)\n",
    "                    # print( mean, var)\n",
    "                    prob[i][cls] *= 1.0 / np.sqrt(2 * math.pi * var) * math.exp(\n",
    "                        -1.0 * math.pow((mean - samples[i, attr]), 2) / var)\n",
    "                # print(i, cls, prob[i][cls])\n",
    "\n",
    "            sumProb = np.sum(prob[i])\n",
    "            if (sumProb > 0.0):\n",
    "                prob[i] = prob[i] / sumProb\n",
    "                predicted[i] = np.argmax(prob[i])\n",
    "            else:\n",
    "                predicted[i] = 10\n",
    "\n",
    "            # print(prob[i])\n",
    "\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to rearrange the outcome of a clustering to match the order of the classes in the training data\n",
    "# note that this is only something to make it easier for you to inspect the results, but probably not a method\n",
    "# you want to integrate in a final system\n",
    "\n",
    "def correctClusters( confM, classes) :\n",
    "    clusterMapping = -1 * np.ones_like(classes)\n",
    "    for k in classes:\n",
    "        temp = list(confM[:, k])\n",
    "        #print(temp)\n",
    "        notDone = True\n",
    "        while notDone:\n",
    "            amax = np.argmax(temp)\n",
    "            if (confM[amax, k] == np.max(confM[amax, :])):\n",
    "                clusterMapping[k] = amax\n",
    "                notDone = False\n",
    "            elif (sum(temp) == 0):\n",
    "                notDone = False\n",
    "            else:\n",
    "                temp[amax] = 0\n",
    "\n",
    "    for k in classes:\n",
    "        if (clusterMapping[k] == -1):\n",
    "            for j in classes:\n",
    "                if j not in clusterMapping:\n",
    "                    clusterMapping[k] = j\n",
    "\n",
    "    return clusterMapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the digits data and normalising to values between 0 and 1\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "\n",
    "num_examples = len(digits.data)\n",
    "num_split = int(0.7 * num_examples)\n",
    "train_features = 1/16. * digits.data[:num_split]\n",
    "train_labels = digits.target[:num_split]\n",
    "test_features = 1/16. *digits.data[num_split:]\n",
    "test_labels = digits.target[num_split:]\n",
    "\n",
    "num_attributes = 64\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# alternatively, loading the MNIST_Light data, which is normalised in the get_data() method already\n",
    "# OBS: this will take a while to run!\n",
    "\n",
    "#mnist = mn.MNISTData('Handout_MNIST/MNIST_Light/*/*.png')\n",
    "#train_features, test_features, train_labels, test_labels = mnist.get_data()\n",
    "#num_attributes = 400\n",
    "#num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "err = 10.76402577683994\n",
      "err = 1.727904974706115\n",
      "err = 1.07209799629523\n",
      "err = 0.6859939745014375\n",
      "Completeness, homogeneity, adj mutual info EM vs labels 0.6650584734269263 0.6320846243363692 0.6429076359534875\n"
     ]
    }
   ],
   "source": [
    "# some dummy call to the EM implementation, here you will have to change things (add parameters, for example)\n",
    "\n",
    "emClusters = EMClusters()\n",
    "emClusters.initialise(train_features, num_classes, num_attributes)\n",
    "clustered, clusterMeansVars = emClusters.fit()\n",
    "\n",
    "completeness_score = metrics.completeness_score(train_labels, clustered)\n",
    "homogeneity_score = metrics.homogeneity_score(train_labels, clustered)\n",
    "mutual_info_score = metrics.adjusted_mutual_info_score(train_labels, clustered)\n",
    "print( \"Completeness, homogeneity, adj mutual info EM vs labels\", completeness_score, homogeneity_score, mutual_info_score)\n",
    "\n",
    "\n",
    "# for index, center in enumerate(clusterMeansVars[:,:,0]):\n",
    "#     img = center.reshape(8, 8)\n",
    "#     plt.figure()\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(img, cmap=plt.cm.gray_r)\n",
    "#     plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below contain a breakdown for what was meant to be executed in a single cell in the handout skeleton. The single cell contained the following description:\n",
    "\n",
    "\"In this cell you should add the k-Means clustering to be able to compare to what you got with EM\n",
    "If you apply \"correctClusters\" from above (works with any confusion matrix), you can even test the results \n",
    "against your or SKLearn's results in a classification attempt.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function for making use of the provided `correctClusters` function output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_correct_labels(predicted_labels, correct_clusters):\n",
    "    kmeans_predicted_corrected = []\n",
    "    for pred in predicted_labels:\n",
    "        kmeans_predicted_corrected.append(correct_clusters[pred])\n",
    "    \n",
    "    return kmeans_predicted_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the set of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [i for i in range(num_classes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX BEFORE CORRECTION\n",
      " [[  0   0   0   0   0   0   0   0   0 125]\n",
      " [ 26   0  63   0   0   0   0  39   1   0]\n",
      " [107   0   9   1   3   3   0   1   0   0]\n",
      " [  0   2   0  13   2 113   0   0   0   0]\n",
      " [  0   0   2   0   7   0 109   6   0   0]\n",
      " [  0  94   0  28   0   2   1   0   1   0]\n",
      " [  0   0   2   0   0   0   0   0 124   1]\n",
      " [  0   1   0   0 123   0   0   1   0   0]\n",
      " [  2   8  61  44   1   1   0   4   1   0]\n",
      " [  0   3   0  97   7   3   0  15   0   0]]\n"
     ]
    }
   ],
   "source": [
    "clustering = KMeans(n_clusters=num_classes)\n",
    "clustering.fit(train_features, train_labels)\n",
    "kmeans_predicted = clustering.predict(train_features)\n",
    "kmeans_confusion_matrix = metrics.confusion_matrix(train_labels, kmeans_predicted)\n",
    "print('CONFUSION MATRIX BEFORE CORRECTION\\n', kmeans_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the metrics of K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-MEANS CLASSIFICATION REPORT\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       125\n",
      "           1       0.46      0.49      0.47       129\n",
      "           2       0.79      0.86      0.83       124\n",
      "           3       0.93      0.87      0.90       130\n",
      "           4       0.99      0.88      0.93       124\n",
      "           5       0.87      0.75      0.80       126\n",
      "           6       0.98      0.98      0.98       127\n",
      "           7       0.86      0.98      0.92       125\n",
      "           8       0.06      0.03      0.04       122\n",
      "           9       0.53      0.78      0.63       125\n",
      "\n",
      "    accuracy                           0.76      1257\n",
      "   macro avg       0.75      0.76      0.75      1257\n",
      "weighted avg       0.75      0.76      0.75      1257\n",
      "\n",
      "CONFUSION MATRIX CORRECT KMEANS\n",
      " [[125   0   0   0   0   0   0   0   0   0]\n",
      " [  0  63  26   0   0   0   1   0  39   0]\n",
      " [  0   9 107   3   0   0   0   3   1   1]\n",
      " [  0   0   0 113   0   2   0   2   0  13]\n",
      " [  0   2   0   0 109   0   0   7   6   0]\n",
      " [  0   0   0   2   1  94   1   0   0  28]\n",
      " [  1   2   0   0   0   0 124   0   0   0]\n",
      " [  0   0   0   0   0   1   0 123   1   0]\n",
      " [  0  61   2   1   0   8   1   1   4  44]\n",
      " [  0   0   0   3   0   3   0   7  15  97]]\n"
     ]
    }
   ],
   "source": [
    "kmeans_correct_clusters = correctClusters(metrics.confusion_matrix(train_labels, kmeans_predicted), classes)\n",
    "kmeans_predicted_corrected = map_correct_labels(kmeans_predicted, kmeans_correct_clusters)\n",
    "\n",
    "kmeans_confusion_matrix_corrected = metrics.confusion_matrix(train_labels, kmeans_predicted_corrected)\n",
    "\n",
    "print('K-MEANS CLASSIFICATION REPORT\\n', metrics.classification_report(train_labels, kmeans_predicted_corrected))\n",
    "print('CONFUSION MATRIX CORRECT KMEANS\\n', kmeans_confusion_matrix_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print('EM CLASSIFICATION REPORT\\n', metrics.classification_report(test_labels, em_predicted))\n",
    "# em_confusion_matrix = metrics.confusion_matrix(train_labels, clustered)\n",
    "# em_correct_clusters = correctClusters(metrics.confusion_matrix(train_labels, clustered), classes)\n",
    "# clusters_corrected = map_correct_labels(clustered, em_correct_clusters)\n",
    "# kmeans_confusion_matrix_corrected = metrics.confusion_matrix(train_labels, clusters_corrected)\n",
    "# print('K-MEANS CLASSIFICATION REPORT\\n', metrics.classification_report(train_labels, clusters_corrected))\n",
    "# print('CONFUSION MATRIX CORRECT EM\\n', kmeans_confusion_matrix_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "err = 10.751480292756897\n",
      "err = 1.1139926726917484\n",
      "err = 1.5287083492756972\n",
      "err = 1.0844638758776464\n",
      "err = 0.842535303247776\n",
      "EM CLASSIFICATION REPORT\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       125\n",
      "           1       0.78      0.33      0.46       129\n",
      "           2       0.60      0.86      0.71       124\n",
      "           3       0.39      0.93      0.55       130\n",
      "           4       0.00      0.00      0.00       124\n",
      "           5       0.90      0.67      0.77       126\n",
      "           6       0.42      0.98      0.58       127\n",
      "           7       0.70      0.90      0.78       125\n",
      "           8       0.00      0.00      0.00       122\n",
      "           9       0.39      0.10      0.15       125\n",
      "\n",
      "    accuracy                           0.58      1257\n",
      "   macro avg       0.52      0.57      0.50      1257\n",
      "weighted avg       0.52      0.58      0.50      1257\n",
      "\n",
      "CONFUSION MATRIX CORRECT EM\n",
      " [[123   0   0   0   2   0   0   0   0   0]\n",
      " [  0  42  35   0   0   0  39  13   0   0]\n",
      " [  0   2 107   5   0   0   5   5   0   0]\n",
      " [  0   0   2 121   0   0   0   5   2   0]\n",
      " [  0   4   0   0   0   0 105   9   0   6]\n",
      " [  0   0   0  37   1  85   3   0   0   0]\n",
      " [  0   0   2   0   1   0 124   0   0   0]\n",
      " [  0   0   0   0   0   0   0 112   0  13]\n",
      " [  0   3  33  46   0   9  22   9   0   0]\n",
      " [  0   3   0 101   0   0   0   8   1  12]]\n"
     ]
    }
   ],
   "source": [
    "# In this cell you should add the k-Means clustering to be able to compare to what you got with EM\n",
    "# If you apply \"correctClusters\" from above (works with any confusion matrix), you can even test the results \n",
    "# against your or SKLearn's results in a classification attempt.\n",
    "\n",
    "# clustering = KMeans(n_clusters=num_classes)\n",
    "# clustering.fit(train_features, train_labels)\n",
    "# kmeans_predicted = clustering.predict(test_features)\n",
    "# # print('K-MEANS CLASSIFICATION REPORT\\n', metrics.classification_report(test_labels, kmeans_predicted))\n",
    "# kmeans_confusion_matrix = metrics.confusion_matrix(test_labels, kmeans_predicted)\n",
    "# kmeans_correct_clusters = correctClusters(metrics.confusion_matrix(test_labels, kmeans_predicted), classes)\n",
    "# kmeans_confusion_matrix_correct = swap_rows(kmeans_confusion_matrix, kmeans_correct_clusters)\n",
    "# print('CONFUSION MATRIX CORRECT KMEANS\\n', kmeans_confusion_matrix_correct)\n",
    "\n",
    "emClusters = EMClusters()\n",
    "emClusters.initialise(train_features, num_classes, num_attributes, blur=0.2)\n",
    "clustered, clusterMeansVars = emClusters.fit()\n",
    "# print('EM CLASSIFICATION REPORT\\n', metrics.classification_report(test_labels, em_predicted))\n",
    "em_confusion_matrix = metrics.confusion_matrix(train_labels, clustered)\n",
    "\n",
    "em_correct_clusters = correctClusters(metrics.confusion_matrix(train_labels, clustered), classes)\n",
    "em_corrected_labels = map_correct_labels(clustered, em_correct_clusters)\n",
    "em_confusion_matrix_corrected = metrics.confusion_matrix(train_labels, em_corrected_labels)\n",
    "# em_confusion_matrix_correct = swap_rows(em_confusion_matrix, em_correct_clusters)\n",
    "print('EM CLASSIFICATION REPORT\\n', metrics.classification_report(train_labels, em_corrected_labels))\n",
    "print('CONFUSION MATRIX CORRECT EM\\n', em_confusion_matrix_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a0b82462ee569e8f9cb1079829714d87cdb26c5d258ceac7e33e44278c9776b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('tf': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
