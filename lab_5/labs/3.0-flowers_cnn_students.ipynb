{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment #5: Convolutional Networks\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Objectives</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of this assignment are to:\n",
    "* Write a program to recognize flowers on images\n",
    "* Learn how to manage an image data set\n",
    "* Apply convolutional networks to images\n",
    "* Understand class activation\n",
    "* Write a short report on your experiments. This report is mandatory to pass the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group will have to:\n",
    "* Write Python programs to recognize the type of flowers in an image.\n",
    "* Experiment different neural architectures and compare the results.\n",
    "\n",
    "Each student will have to:\n",
    "* Run at home Chollet's notebook: `chapter09_part03_interpreting-what-convnets-learn.ipynb`. You can download from https://github.com/fchollet/deep-learning-with-python-notebooks.\n",
    "* Write an individual report on these experiments.\n",
    "\n",
    "While not compulsory, I highly recommend that you use Google colab and run your program on a GPU (Unless you have a GPU on your machine). This is something you select when you run the notebook (runtime type). You will have to create a Google account for this: https://colab.research.google.com\n",
    "\n",
    "As there are a few parameterization steps, I recommend that you start this lab as early as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Register to Kaggle (https://www.kaggle.com/) to collect the datset used in this experiment. It is free and you will have access to lots of datasets. If you do not want to register, go to step 2.\n",
    "2. Download the Flower corpus:\n",
    "   * Either from Kaggle if you have registered (https://www.kaggle.com/alxmamaev/flowers-recognition). \n",
    "   * Or use a local copy, `flower-recognition.zip`, in the `datasets` folder in canvas.\n",
    "3. Split randomly your dataset into training, validation, and test sets: Use a 60/20/20 ratio. You will read all the file names and create a list of pairs, (file_name, category). You will then shuffle your list and save your partition of the data. To speed up the lab, you can also:\n",
    "   * Start with the partition available in the canvas folder (`flower_split.zip`); or\n",
    "   * Run the code in the cells below (recommended).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# The machine name. Use the name colab is you run the code from Google colab\n",
    "machine_name = 'filip'\n",
    "\n",
    "# To create the same dataset\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Google colab (recommended). Use the GPU accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "if machine_name == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here write the path to your dataset\n",
    "if machine_name == 'vilde':\n",
    "    base = '/home/pierre/Cours/EDAN95/datasets/'\n",
    "elif machine_name == 'filip':\n",
    "    base = 'C:\\\\Users\\\\filip\\\\Documents\\\\Högskola\\\\[EDAN95] - Applied Machine Learning\\\\lab_5\\\\'\n",
    "elif machine_name == 'colab': # If you run your notebook with colab\n",
    "    base = '/content/drive/My Drive/Colab Notebooks/'\n",
    "else: # If you run your notebook on your machine\n",
    "    base = 'your folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image types: ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\filip\\\\Documents\\\\Högskola\\\\[EDAN95] - Applied Machine Learning\\\\lab_5\\\\flowers\\\\daisy',\n",
       " 'C:\\\\Users\\\\filip\\\\Documents\\\\Högskola\\\\[EDAN95] - Applied Machine Learning\\\\lab_5\\\\flowers\\\\dandelion',\n",
       " 'C:\\\\Users\\\\filip\\\\Documents\\\\Högskola\\\\[EDAN95] - Applied Machine Learning\\\\lab_5\\\\flowers\\\\rose',\n",
       " 'C:\\\\Users\\\\filip\\\\Documents\\\\Högskola\\\\[EDAN95] - Applied Machine Learning\\\\lab_5\\\\flowers\\\\sunflower',\n",
       " 'C:\\\\Users\\\\filip\\\\Documents\\\\Högskola\\\\[EDAN95] - Applied Machine Learning\\\\lab_5\\\\flowers\\\\tulip']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset_dir = os.path.join(base, 'flowers')\n",
    "dataset = os.path.join(base, 'flowers_split')\n",
    "\n",
    "train_dir = os.path.join(dataset, 'train')\n",
    "validation_dir = os.path.join(dataset, 'validation')\n",
    "test_dir = os.path.join(dataset, 'test')\n",
    "\n",
    "categories = os.listdir(original_dataset_dir)\n",
    "categories = [category for category in categories if not category.startswith('.')]\n",
    "print('Image types:', categories)\n",
    "data_folders = [os.path.join(original_dataset_dir, category) for category in categories]\n",
    "data_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the (image, label) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100080576_f52e8ee070_n.jpg', 'daisy'),\n",
       " ('10140303196_b88d3d6cec.jpg', 'daisy'),\n",
       " ('10172379554_b296050f82_n.jpg', 'daisy'),\n",
       " ('10172567486_2748826a8b.jpg', 'daisy'),\n",
       " ('10172636503_21bededa75_n.jpg', 'daisy'),\n",
       " ('102841525_bd6628ae3c.jpg', 'daisy'),\n",
       " ('10300722094_28fa978807_n.jpg', 'daisy'),\n",
       " ('1031799732_e7f4008c03.jpg', 'daisy'),\n",
       " ('10391248763_1d16681106_n.jpg', 'daisy'),\n",
       " ('10437754174_22ec990b77_m.jpg', 'daisy')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "for folder, category in zip(data_folders, categories):\n",
    "    images = os.listdir(folder)\n",
    "    images = [image for image in images if not image.startswith('.')]\n",
    "    pairs.extend([(image, category) for image in images])\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a split. We will run this part only once to create the partition into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2590\n",
      "863\n",
      "864\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random.shuffle(pairs)\n",
    "img_nbr = len(pairs)\n",
    "train_images = pairs[0:int(0.6 * img_nbr)]\n",
    "val_images = pairs[int(0.6 * img_nbr):int(0.8 * img_nbr)]\n",
    "test_images = pairs[int(0.8 * img_nbr):]\n",
    "\n",
    "# print(train_images)\n",
    "print(len(train_images))\n",
    "print(len(val_images))\n",
    "print(len(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the three subsets. We will run this part only once to create the partition into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for image, label in tqdm(train_images):\n",
    "        src = os.path.join(original_dataset_dir, label, image)\n",
    "        dst = os.path.join(train_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    for image, label in tqdm(val_images):\n",
    "        src = os.path.join(original_dataset_dir, label, image)\n",
    "        dst = os.path.join(validation_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    for image, label in tqdm(test_images):\n",
    "        src = os.path.join(original_dataset_dir, label, image)\n",
    "        dst = os.path.join(test_dir, label, image)\n",
    "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple convolutional network and train a model with the training set. You can start from the architecture proposed by Chollet, Listing 8.7 (in Chollet's notebook chapter 8), and a small number of epochs. Use the `Rescaling` layer to scale your images as in the book:\n",
    "```\n",
    "layer.Rescaling(1.255)\n",
    "```\n",
    "* You will need to modify some parameters so that your network handles multiple classes.\n",
    "* You will report the training and validation losses and accuracies and comment on the possible overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.image import ResizeMethod\n",
    "\n",
    "EPOCHS = 30\n",
    "OPTIMIZER = 'rmsprop'\n",
    "# The pretrained network\n",
    "PRETRAINED = 'INCEPTION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3632 files belonging to 5 classes.\n",
      "Found 1555 files belonging to 5 classes.\n",
      "Found 1563 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    os.path.join(base ,\n",
    "        \"flowers_split\" ,\n",
    "        \"train\")\n",
    "    ,\n",
    "    image_size=(255, 255),\n",
    "    batch_size=64)\n",
    "\n",
    "validation_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    os.path.join(base, \"flowers_split\", \"validation\"),\n",
    "    image_size=(255, 255),\n",
    "    batch_size=64)\n",
    "\n",
    "test_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    os.path.join(base, \"flowers_split\", \"test\"),\n",
    "    image_size=(255, 255),\n",
    "    batch_size=64,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the network and compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(255, 255, 3))\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(len(test_dataset.class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 255, 255, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 253, 253, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 126, 126, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 124, 124, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 60, 60, 128)       73856     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 460800)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 2304005   \n",
      "=================================================================\n",
      "Total params: 2,397,253\n",
      "Trainable params: 2,397,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create now the data readers for the training, validation, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now fit your model. Before, you will define a callback as in Chollet's book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"lab5.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fit your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 [==============================] - 124s 2s/step - loss: 370.5976 - accuracy: 0.2536 - val_loss: 1.4521 - val_accuracy: 0.4283\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 122s 2s/step - loss: 6.3164 - accuracy: 0.4389 - val_loss: 1.3204 - val_accuracy: 0.5273\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 128s 2s/step - loss: 17.6193 - accuracy: 0.4078 - val_loss: 1.4582 - val_accuracy: 0.5055\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 128s 2s/step - loss: 10.4818 - accuracy: 0.5424 - val_loss: 1.5472 - val_accuracy: 0.5762\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 126s 2s/step - loss: 36.2846 - accuracy: 0.5677 - val_loss: 91.0303 - val_accuracy: 0.1904\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 125s 2s/step - loss: 5.1523 - accuracy: 0.6847 - val_loss: 6.6365 - val_accuracy: 0.4791\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 128s 2s/step - loss: 15.9234 - accuracy: 0.6886 - val_loss: 1.7699 - val_accuracy: 0.6862\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 128s 2s/step - loss: 0.9022 - accuracy: 0.8263 - val_loss: 8.1479 - val_accuracy: 0.4617\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 124s 2s/step - loss: 13.1101 - accuracy: 0.7965 - val_loss: 2.1898 - val_accuracy: 0.7209\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 121s 2s/step - loss: 10.7610 - accuracy: 0.7574 - val_loss: 2.4557 - val_accuracy: 0.7061\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the fitting performance over epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Chollet's examples, show the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now apply now your network to the test set. Read the best model from your checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 10s 419ms/step - loss: 1.3204 - accuracy: 0.1170\n",
      "Test accuracy: 0.117\n"
     ]
    }
   ],
   "source": [
    "test_model = keras.models.load_model(\"lab5.keras\")\n",
    "test_loss_1, test_acc_1 = test_model.evaluate(validation_dataset)\n",
    "print(f\"Test accuracy: {test_acc_1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the test set classes. Call the matrix of prediction probabilities `Y_pred` and the predicted classes `y_pred`. While predicting, extract the annotations that you will call `y_true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = test_model.predict(test_dataset)\n",
    "y_pred = [np.argmax(probability) for probability in Y_pred]\n",
    "y_true = np.concatenate([y for x, y in test_dataset], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19442478, 0.2114404 , 0.19499424, 0.19508348, 0.20405711],\n",
       "       [0.31110004, 0.11954392, 0.36007163, 0.06494223, 0.14434223],\n",
       "       [0.757309  , 0.06603438, 0.12301759, 0.006983  , 0.046656  ],\n",
       "       [0.34597012, 0.13018872, 0.21812524, 0.12543905, 0.18027689],\n",
       "       [0.21109273, 0.22403704, 0.17636569, 0.19906221, 0.18944228]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0, 0, 1, 0, 4, 2, 2, 2, 4, 0, 2, 2, 0, 2, 1, 2, 0, 0]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model and report the loss and accuracy using Keras `evaluate()` function. You will store them in the `test_loss_1` and `test_acc_1` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3204034566879272, 0.11704180389642715)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_1, test_acc_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the accuracy as well as the classification report. Use sklearn functions this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11704180389642715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.52      0.60       316\n",
      "           1       0.54      0.45      0.49       374\n",
      "           2       0.45      0.59      0.51       282\n",
      "           3       0.51      0.73      0.60       250\n",
      "           4       0.54      0.48      0.51       341\n",
      "\n",
      "    accuracy                           0.54      1563\n",
      "   macro avg       0.55      0.55      0.54      1563\n",
      "weighted avg       0.56      0.54      0.54      1563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(test_acc_1)\n",
    "print(metrics.classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the confusion matrix. Use a sklearn function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[163,  36,  49,  28,  40],\n",
       "       [ 23, 169,  69,  75,  38],\n",
       "       [ 25,  34, 166,  19,  38],\n",
       "       [  4,  29,  12, 183,  22],\n",
       "       [ 11,  43,  70,  55, 162]], dtype=int64)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to improve your model by modifying some parameters and evaluate your network again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Image Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flower dataset is relatively small. A way to expand such datasets is to generate artificial images by applying small transformations to existing images. Keras provides built-in layers for this. You will reuse them and apply it to the flower data set.\n",
    "1. Using the network from the previous exercise, apply some transformations to your images. You can start from Chollet, Listing 8.13 (in notebook 08 also).\n",
    "2. Report the training and validation losses and accuracies and comment on the possible overfit.\n",
    "3. Apply your network to the test set and report the accuracy as well as the confusion matrix you obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a sequence of transformation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "        layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def data_augmentation(x):\n",
    "    \n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "            layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "            layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "        ]\n",
    "    )(x)\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model including these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (sequential_1/random_rotation_1/rotation_matrix_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31860/199993747.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    926\u001b[0m                                                 input_list)\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1115\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m               \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    926\u001b[0m                                                 input_list)\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1115\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m               \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\preprocessing\\image_preprocessing.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    820\u001b[0m           interpolation=self.interpolation)\n\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m     output = tf_utils.smart_cond(training, random_rotated_inputs,\n\u001b[0m\u001b[0;32m    823\u001b[0m                                  lambda: inputs)\n\u001b[0;32m    824\u001b[0m     \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msmart_cond\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m     62\u001b[0m     return control_flow_ops.cond(\n\u001b[0;32m     63\u001b[0m         pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[1;32m---> 64\u001b[1;33m   return smart_module.smart_cond(\n\u001b[0m\u001b[0;32m     65\u001b[0m       pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[1;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mpred_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\preprocessing\\image_preprocessing.py\u001b[0m in \u001b[0;36mrandom_rotated_inputs\u001b[1;34m()\u001b[0m\n\u001b[0;32m    816\u001b[0m       return transform(\n\u001b[0;32m    817\u001b[0m           \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m           \u001b[0mget_rotation_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mangles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_hd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_wd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m           \u001b[0mfill_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_mode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m           interpolation=self.interpolation)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\preprocessing\\image_preprocessing.py\u001b[0m in \u001b[0;36mget_rotation_matrix\u001b[1;34m(angles, image_height, image_width, name)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mangles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m             \u001b[0my_offset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_angles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         ],\n\u001b[0;32m    724\u001b[0m         axis=1)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2746\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2747\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2748\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_zeros_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2749\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[1;34m(shape, dtype, name)\u001b[0m\n\u001b[0;32m   2792\u001b[0m           \u001b[1;31m# Create a constant if it won't be very big. Otherwise create a fill\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2793\u001b[0m           \u001b[1;31m# op to prevent serialized GraphDefs from becoming too large.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2794\u001b[1;33m           \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2795\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2796\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_constant_if_small\u001b[1;34m(value, shape, dtype, name)\u001b[0m\n\u001b[0;32m   2730\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2731\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2732\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2733\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2734\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   3049\u001b[0m     \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m-> 3051\u001b[1;33m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[0;32m   3052\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0;32m   3053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m     raise NotImplementedError(\n\u001b[0m\u001b[0;32m    846\u001b[0m         \u001b[1;34m\"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m         \u001b[1;34m\" This error may indicate that you're trying to pass a Tensor to\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (sequential_1/random_rotation_1/rotation_matrix_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(255, 255, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(len(test_dataset.class_names), activation=\"softmax\")(x)\n",
    "model_augmented = keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_augmented.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_augmented.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a callback to save your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"lab5-augmented.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fit your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_augmented = model_augmented.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analyzing the fitting performance over epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the training accuracy along with the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history_augmented.history[\"accuracy\"]\n",
    "val_accuracy = history_augmented.history[\"val_accuracy\"]\n",
    "loss = history_augmented.history[\"loss\"]\n",
    "val_loss = history_augmented.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\"lab5-augmented.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run a model evaluation and store the loss and accuracy in the `test_loss_2` and `test_acc_2` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_2, test_acc_2 = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc_2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_2, test_acc_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using a Pretrained Convolutional Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some research teams have trained convolutional neural networks on much larger datasets. We have seen during the lecture that the networks can model conceptual patterns as they go through the layers. This was identified by Le Cun in his first experiments [http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/). In this last part, you will train classifiers on top of a pretrained convolutional base.\n",
    "1. Train your network and report the training and validation losses and accuracies.\n",
    "2. Apply your network to the test set and report the accuracy as well as the confusion matrix you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRETRAINED == 'VGG':\n",
    "    conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(180, 180, 3))\n",
    "else:\n",
    "    conv_base = InceptionV3(weights='imagenet',\n",
    "                        include_top=False,\n",
    "                        input_shape=(180, 180, 3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Chollet, Listing 8.20 (in Chollet's notebook 08), you will program a `get_features_and_labels()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
    "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
    "test_features, test_labels =  get_features_and_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a network that consists of the Inception V3 convolutional base and two dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a callback to save your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the fitting performance over epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the training accuracy along with the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your best model from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model and store the loss and accuracy in `test_loss_3`, `test_acc_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_3, test_acc_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Pretrained Convolutional Base with Image Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Modify your program to include an image transformer. Train a new model. As a hint, use nadam or rmsprop as optimizer. Rescaling seems to lower the accuracy. Do not use it in your first attempts.\n",
    "2. Apply your network to the test set and report the accuracy as well as the confusion matrix you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRETRAINED == 'VGG':\n",
    "    conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(180, 180, 3))\n",
    "else:\n",
    "    conv_base = InceptionV3(weights='imagenet',\n",
    "                        include_top=False,\n",
    "                        input_shape=(180, 180, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to try this first, and possibly modify it to improve the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write you code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write you code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a callback to save your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the fitting performance over epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the training accuracy along with the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate it and store the loss and accuracy in `test_loss_4` and `test_acc_4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_4, test_acc_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predicting the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply now your network to the test set. You will collect `y_true` and `y_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the accuracy as well as the classification report. Use sklearn functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing the Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass the assignment, you need to reach an accuracy of 75 (even 80 ideally) with your best network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_loss_1, test_acc_2), (test_loss_2, test_acc_2), (test_loss_3, test_acc_3), (test_loss_4, test_acc_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will write a short report of about two pages on your experiments:\n",
    "1. You will describe all the architectures you designed and the results you obtained. You will summarize the results in a table. Your description should be one-page long;\n",
    "2. You will run Chollet's notebook chapter09_part3 and read the article _Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization_ by Selvaraju et al. From this, you will reformulate and comment the paragraph on _Visualizing heatmaps of class activation_ in the notebook. Your analysis should be about one-page long.\n",
    "3. You will run it on one of your flower images.\n",
    "\n",
    "You will submit your report as well as your notebook through Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
