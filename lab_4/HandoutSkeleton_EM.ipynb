{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import OrderedDict\n",
    "from sklearn import datasets, metrics\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import Handout_MNIST.MNIST as mn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell you will have to add a suitable intialisation for the EM-algorithm to find good \"clusters\"\n",
    "# note that \"cluster\" is a bit misleading, as only the degree of belonging to a distribution is evaluated \n",
    "# for each sample, and not an absolute assignment to a cluster is made\n",
    "\n",
    "class EMClusters:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__numOfClasses = 0\n",
    "        self.__numOfAttributes = 0\n",
    "\n",
    "    # Initialises for EM to work\n",
    "    # should be called with a suitable smoothing factor blur, but the default does at least something.\n",
    "    def initialise(self, data, num_classes, num_attributes, blur=0.1):\n",
    "        self.__blur = blur\n",
    "        self.__data = data\n",
    "        self.__numOfAttributes = num_attributes\n",
    "  \n",
    "        self.__numOfClasses = num_classes\n",
    "\n",
    "        self.__clusterMeansAndCovs = np.ones((self.__numOfClasses, self.__numOfAttributes, 2), dtype=float)\n",
    "        self.__priors = np.ones(self.__numOfClasses)\n",
    "\n",
    "        # start of your code\n",
    "        ###\n",
    "        #\n",
    "        # Here, you should now fill in something more sensible, as the code above is only a placeholder solution...\n",
    "        # You need to find initial clusters and set start values for the priors, means and (co)variances that would\n",
    "        # end up in the following arrays:\n",
    "        #\n",
    "        # self.__priors              - is of dimension K x 1 and holds K values, K the number of clusters / classes\n",
    "        # self.__clusterMeansAndCovs - is of dimensions K x I x 2 and holds for each combination of class k\n",
    "        #                              and attribute j the mean in [k, j, 0] and the covariance (only for jj,\n",
    "        #                              as we assume conditional independence given the class) in [k, j, 1]\n",
    "        #\n",
    "        ###\n",
    "        # end of your code\n",
    "\n",
    "        \n",
    "    # the actual EM algorithm, should be called with a suitable eps for a stop criterion\n",
    "    # feel free to change (optimize) the implementation, but you do not have to do that\n",
    "    def fit(self, eps=1.0):\n",
    "        epsilon = eps\n",
    "\n",
    "        print(self.__numOfAttributes)\n",
    "\n",
    "        # print(self.__clusterMeansAndCovs[:,1,:])\n",
    "\n",
    "        resp = np.zeros((len(self.__data), self.__numOfClasses))\n",
    "        r_k = np.zeros(self.__numOfClasses)\n",
    "        notDone = True\n",
    "        while notDone:\n",
    "            # print(self.__clusterMeansAndCovs)\n",
    "            # E-step\n",
    "            for i in range(len(self.__data)):\n",
    "                probs = self.__priors.copy()\n",
    "                for k in range(self.__numOfClasses):\n",
    "\n",
    "                    for attr in range(self.__numOfAttributes):\n",
    "                        probs[k] *= 1.0 / np.sqrt(\n",
    "                            2 * math.pi * (self.__clusterMeansAndCovs[k, attr, 1] + self.__blur)) * math.exp(\n",
    "                            -1.0 * math.pow((self.__clusterMeansAndCovs[k, attr, 0] - self.__data[i, attr]), 2) / (\n",
    "                                        self.__clusterMeansAndCovs[k, attr, 1] + self.__blur))\n",
    "\n",
    "                resp[i, :] = probs / sum(probs)\n",
    "                # print(resp[i,:])\n",
    "            \n",
    "            # M-step\n",
    "            newClusterMeansAndCovs = np.zeros((self.__numOfClasses, self.__numOfAttributes, 2))\n",
    "            for k in range(self.__numOfClasses):\n",
    "                r_k[k] = sum(resp[:, k])\n",
    "                self.__priors[k] = 1 / len(self.__data) * r_k[k]\n",
    "\n",
    "                for j in range(self.__numOfAttributes):\n",
    "                    means = sum(resp[:, k] * self.__data[:, j]) / r_k[k]\n",
    "\n",
    "                    newClusterMeansAndCovs[k, j, 0] = means\n",
    "                    newClusterMeansAndCovs[k, j, 1] = sum(resp[:, k] * self.__data[:, j] * self.__data[:, j]) \\\n",
    "                                                      / r_k[k] - newClusterMeansAndCovs[k, j, 0] ** 2\n",
    "\n",
    "            print(newClusterMeansAndCovs[:, :, 1])\n",
    "\n",
    "            err = np.linalg.norm(self.__clusterMeansAndCovs[:, :, 0] - newClusterMeansAndCovs[:, :, 0])\n",
    "\n",
    "            self.__clusterMeansAndCovs = newClusterMeansAndCovs.copy()\n",
    "            print(\"err = \" + str(err))\n",
    "            if err <= epsilon:\n",
    "                notDone = False\n",
    "                \n",
    "                \n",
    "        # produce \"clusters\", i.e, assign the samples to \"their\" gaussian\n",
    "        clustered = 10 * np.ones(len(self.__data), dtype=int)\n",
    "        for i in range(len(self.__data)):\n",
    "            probs = self.__priors.copy()\n",
    "            for k in range(self.__numOfClasses):\n",
    "\n",
    "                for attr in range(self.__numOfAttributes):\n",
    "                    probs[k] *= 1.0 / np.sqrt(2 * math.pi * (self.__clusterMeansAndCovs[k, attr, 1] + self.__blur)) \\\n",
    "                                * math.exp(\n",
    "                        -1.0 * math.pow((self.__clusterMeansAndCovs[k, attr, 0] - self.__data[i, attr]), 2) \\\n",
    "                        / (self.__clusterMeansAndCovs[k, attr, 1] + self.__blur))\n",
    "\n",
    "            sumProb = np.sum(probs)\n",
    "            if (sumProb > 0.0):\n",
    "                probs = probs / sumProb\n",
    "                clustered[i] = np.argmax(probs)\n",
    "\n",
    "        return clustered, self.__clusterMeansAndCovs\n",
    "\n",
    "    # prediction uses the found gaussians to compute the likelihoods in a GNBC\n",
    "    def predict(self, samples):\n",
    "        predicted = [None for i in range(len(samples))]\n",
    "        prob = [[self.__priors[cls] for cls in range(self.__numOfClasses)] for i in range(len(samples))]\n",
    "        # print(prob)\n",
    "        for i in range(len(samples)):\n",
    "            for cls in range(self.__numOfClasses):\n",
    "                for attr in range(self.__numOfAttributes):\n",
    "                    (mean, var) = (\n",
    "                    self.__clusterMeansAndCovs[cls, attr, 0], self.__clusterMeansAndCovs[cls, attr, 1] + self.__blur)\n",
    "                    # print( mean, var)\n",
    "                    prob[i][cls] *= 1.0 / np.sqrt(2 * math.pi * var) * math.exp(\n",
    "                        -1.0 * math.pow((mean - samples[i, attr]), 2) / var)\n",
    "                # print(i, cls, prob[i][cls])\n",
    "\n",
    "            sumProb = np.sum(prob[i])\n",
    "            if (sumProb > 0.0):\n",
    "                prob[i] = prob[i] / sumProb\n",
    "                predicted[i] = np.argmax(prob[i])\n",
    "            else:\n",
    "                predicted[i] = 10\n",
    "\n",
    "            # print(prob[i])\n",
    "\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to rearrange the outcome of a clustering to match the order of the classes in the training data\n",
    "# note that this is only something to make it easier for you to inspect the results, but probably not a method\n",
    "# you want to integrate in a final system\n",
    "\n",
    "def correctClusters( confM, classes) :\n",
    "    clusterMapping = -1 * np.ones_like(classes)\n",
    "    for k in classes:\n",
    "        temp = list(confM[:, k])\n",
    "        #print(temp)\n",
    "        notDone = True\n",
    "        while notDone:\n",
    "            amax = np.argmax(temp)\n",
    "            if (confM[amax, k] == np.max(confM[amax, :])):\n",
    "                clusterMapping[k] = amax\n",
    "                notDone = False\n",
    "            elif (sum(temp) == 0):\n",
    "                notDone = False\n",
    "            else:\n",
    "                temp[amax] = 0\n",
    "\n",
    "    for k in classes:\n",
    "        if (clusterMapping[k] == -1):\n",
    "            for j in classes:\n",
    "                if j not in clusterMapping:\n",
    "                    clusterMapping[k] = j\n",
    "\n",
    "    return clusterMapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the digits data and normalising to values between 0 and 1\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "\n",
    "num_examples = len(digits.data)\n",
    "num_split = int(0.7 * num_examples)\n",
    "train_features = 1/16. * digits.data[:num_split]\n",
    "train_labels = digits.target[:num_split]\n",
    "test_features = 1/16. *digits.data[num_split:]\n",
    "test_labels = digits.target[num_split:]\n",
    "\n",
    "num_attributes = 64\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# alternatively, loading the MNIST_Light data, which is normalised in the get_data() method already\n",
    "# OBS: this will take a while to run!\n",
    "\n",
    "#mnist = mn.MNISTData('Handout_MNIST/MNIST_Light/*/*.png')\n",
    "#train_features, test_features, train_labels, test_labels = mnist.get_data()\n",
    "#num_attributes = 400\n",
    "#num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some dummy call to the EM implementation, here you will have to change things (add parameters, for example)\n",
    "\n",
    "emClusters = EMClusters()\n",
    "emClusters.initialise(train_features, num_classes, num_attributes)\n",
    "clustered, clusterMeansVars = emClusters.fit()\n",
    "\n",
    "completeness_score = metrics.completeness_score(train_labels, clustered)\n",
    "homogeneity_score = metrics.homogeneity_score(train_labels, clustered)\n",
    "mutual_info_score = metrics.adjusted_mutual_info_score(train_labels, clustered)\n",
    "print( \"Completeness, homogeneity, adj mutual info EM vs labels\", completeness_score, homogeneity_score, mutual_info_score)\n",
    "\n",
    "\n",
    "for index, center in enumerate(clusterMeansVars[:,:,0]):\n",
    "    img = center.reshape(8, 8)\n",
    "    plt.figure()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap=plt.cm.gray_r)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell you should add the k-Means clustering to be able to compare to what you got with EM\n",
    "# If you apply \"correctClusters\" from above (works with any confusion matrix), you can even test the results \n",
    "# against your or SKLearn's results in a classification attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
